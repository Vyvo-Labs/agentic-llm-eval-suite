# llm-eval-suite

Env-driven LLM benchmark + hybrid judge suite.

This project benchmarks candidate models using local YAML datasets and computes
scores from deterministic checks plus optional LLM-judge rubric scoring.

## Key behavior

- Loads `.env` from repo root (process env overrides file values).
- Uses OpenAI-compatible provider mapping aligned with `external_repo` conventions:
  - `openai`, `groq`, `openrouter`, `fireworks`, `together`, `cerebras`
- Candidate model matrix comes from provider presets.
- Judge model is dedicated and independently configurable.
- Output artifacts:
  - `results.json`
  - `leaderboard.md`
  - `leaderboard.html`
  - `leaderboard_detailed.pdf` (generated by default; use `--no-detailed-pdf` to skip)
  - `leaderboard_assets/leaderboard.pdf`
  - `leaderboard_assets/leaderboard.png`
  - `raw_responses.jsonl`
  - `reports/history.html` (day-by-day dashboard across runs with historical leaderboard + winner metrics)
  - `reports/history_assets/history.pdf`
  - `reports/history_assets/history.png`

## Install

```bash
uv sync
```

## Commands

```bash
# Run full benchmark (default datasets)
uv run eval-suite run

# Run filtered benchmark
uv run eval-suite run --provider openai --model gpt-5 --max-cases 10

# List resolved candidate models from env/presets
uv run eval-suite list-models

# Rebuild markdown + HTML reports from prior run
uv run eval-suite report --input reports/<run_id>/results.json

# Export a detailed single-run PDF report for leadership sharing
uv run eval-suite report --input reports/<run_id>/results.json --detailed-pdf

# Limit historical prior runs included in detailed PDF reliability scoring
uv run eval-suite report --input reports/<run_id>/results.json --detailed-pdf --history-max-runs 30

# Include optional raw outputs appendix in the default run detailed PDF
uv run eval-suite run --max-cases 10 --include-raw-output

# Skip detailed PDF generation for a benchmark run
uv run eval-suite run --no-detailed-pdf

# Rebuild day-by-day dashboard across reports/
uv run eval-suite history

# Optional custom output paths
uv run eval-suite report --input reports/<run_id>/results.json --output reports/custom.md --html-output reports/custom.html
uv run eval-suite report --input reports/<run_id>/results.json --detailed-pdf-output reports/cto-report.pdf
uv run eval-suite run --history-reports-dir reports --history-max-runs 50
```

Detailed PDF export uses Playwright when available and fails soft: markdown/HTML artifacts are still written if PDF rendering is unavailable.
Detailed PDF now includes a historical reliability section that aggregates prior `results.json` scores from `reports/` (configurable via `--history-reports-dir` and `--history-max-runs`).

## Dataset format

Datasets live in `datasets/*.yaml` and use schema version 1:

```yaml
schema_version: 1
cases:
  - id: "example"
    name: "Example case"
    type: "single_turn" # or multi_turn
    category: "general"
    tags: ["tag1"]
    input:
      user: "Prompt"
      # or messages: [{role, content}, ...]
    expected:
      exact: ["optional exact match"]
      regex: ["optional regex"]
      must_include: ["required phrase"]
      json_valid: false
    judge_rubric:
      criteria:
        - "Criterion text"
      force: false
    weights:
      deterministic: 0.5
      judge: 0.5
```

## Important env vars

### Candidate matrix

- `EVAL_PROVIDER_PRESET` (`all` or comma list of providers, default: `openrouter`)
- `EVAL_MAX_MODELS_PER_PROVIDER` (default: `20`)
- `EVAL_PRESET_MODELS_OPENAI` (optional comma list override)
- `EVAL_PRESET_MODELS_GROQ`
- `EVAL_PRESET_MODELS_OPENROUTER`
- `EVAL_PRESET_MODELS_FIREWORKS`
- `EVAL_PRESET_MODELS_TOGETHER`
- `EVAL_PRESET_MODELS_CEREBRAS`

Legacy agent env aliases (auto-mapped when `EVAL_*` values are not set):

```bash
LLM_PROVIDER=anthropic
LLM_MODEL=claude-4-5-haiku
TOOL_MESSAGE_MODEL=claude-4-5-haiku
LLM_REASONING_EFFORT=none
```

Notes:
- `LLM_PROVIDER=anthropic` maps to `openrouter` for this suite.
- `LLM_MODEL` and `TOOL_MESSAGE_MODEL` do not narrow candidates by default.
- Set `EVAL_LEGACY_MODEL_PIN=true` to use `LLM_MODEL` + `TOOL_MESSAGE_MODEL` as a pinned candidate list.

Default OpenAI preset models:
- `gpt-5-mini`
- `gpt-5`
- `gpt-5.2`
- `gpt-5-mini/minimal`
- `gpt-5.2/none`

Default OpenRouter preset models:
- `moonshotai/kimi-k2.5`
- `z-ai/glm-5`
- `z-ai/glm-4.7`
- `qwen/qwen3.5-397b-a17b`
- `deepseek/deepseek-v3.2`
- `anthropic/claude-haiku-4.5`
- `anthropic/claude-sonnet-4.5`
- `anthropic/claude-opus-4.5`
- `openai/gpt-5-mini/minimal`
- `openai/gpt-5.2`
- `openai/gpt-5.2/none`
- `openai/gpt-4.1`
- `openai/gpt-4.1-mini`
- `z-ai/glm-4.7-flash`
- `MiniMaxAI/MiniMax-M2.5`
- `minimax/minimax-m2-her`
- `stepfun/step-3.5-flash`
- `xiaomi/mimo-v2-flash`
- `nvidia/nemotron-3-nano-30b-a3b`
- `meituan/longcat-flash-chat`

### Runtime

- `EVAL_CONCURRENCY` (default: `4`)
- `EVAL_TIMEOUT_S` (default: `90`)
- `EVAL_MAX_COMPLETION_TOKENS` (default: `512`)
- `EVAL_REASONING_EFFORT`
- `EVAL_TEMPERATURE`
- For `gpt-5.2`, set `EVAL_REASONING_EFFORT=none` for minimal-reasoning runs.
- You can encode reasoning effort in model names with a suffix tag:
  - `gpt-5-mini/minimal`
  - `openai/gpt-5.2/none`
- Legacy aliases still work:
  - `openai-5.2-none` and `openai/gpt-5.2-none` -> `reasoning_effort=none`
  - `openai-5-mini-minimal` and `openai/gpt-5-mini-minimal` -> `reasoning_effort=minimal`
- `EVAL_CACHE_ENABLED` (`true` by default)
- `EVAL_CACHE_DIR` (default: `.cache/llm_eval_suite`)
- `EVAL_OUTPUT_DIR` (default: `reports`)
- `EVAL_EXPORT_PAGE_ASSETS` (`true` by default; set `0`/`false` to disable HTML->PDF/PNG export)

### Judge model

- `EVAL_JUDGE_PROVIDER` (default: `openai`)
- `EVAL_JUDGE_MODEL` (default: `gpt-5.2`)
- `EVAL_JUDGE_BASE_URL` (optional)
- `EVAL_JUDGE_API_KEY` (optional override; otherwise provider key is used)
- `EVAL_JUDGE_TIMEOUT_S` (default: `90`)
- `EVAL_JUDGE_MAX_COMPLETION_TOKENS` (default: `400`)
- `EVAL_JUDGE_REASONING_EFFORT` (default: `xhigh`)

### Provider keys

- `OPENAI_API_KEY`
- `GROQ_API_KEY`
- `OPENROUTER_API_KEY`
- `FIREWORKS_API_KEY`
- `TOGETHER_API_KEY`
- `CEREBRAS_API_KEY`

## Notes

- Secrets are never written to output artifacts.
- Providers missing required API keys are skipped with warnings.
- CI can consume `results.json` while keeping benchmark reporting non-blocking.

## CI (non-blocking)

- Workflow: `.github/workflows/llm-eval-report.yml`
- Script: `scripts/ci_benchmark.sh`
- Behavior:
  - Uses `OPENAI_API_KEY` from GitHub secrets when available.
  - Runs a small benchmark sample (`EVAL_CI_MAX_CASES`, default `2`).
  - Uploads `reports/` artifacts.
  - Does not fail the pipeline (`continue-on-error: true`).
